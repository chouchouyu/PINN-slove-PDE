{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40bfb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "It: 0, Loss: 1.446e+00, Y0: 1.251, Time: 0.09, Learning Rate: 1.000e-03\n",
      "It: 100, Loss: 6.273e-01, Y0: 0.186, Time: 1.78, Learning Rate: 1.000e-03\n",
      "It: 200, Loss: 7.903e-01, Y0: -0.198, Time: 1.71, Learning Rate: 1.000e-03\n",
      "It: 300, Loss: 3.111e-01, Y0: 0.294, Time: 1.77, Learning Rate: 1.000e-03\n",
      "It: 400, Loss: 3.780e-01, Y0: 0.036, Time: 1.71, Learning Rate: 1.000e-03\n",
      "It: 500, Loss: 3.980e-01, Y0: 0.059, Time: 1.76, Learning Rate: 1.000e-03\n",
      "It: 600, Loss: 4.310e-01, Y0: 0.169, Time: 1.73, Learning Rate: 1.000e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m tot \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 34\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# model.load_model(\"models/CallOption4-256XVAPaper.pth\")\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n",
      "File \u001b[0;32m~/PINN-slove-PDE/cqf_0_FBSNNs.py:257\u001b[0m, in \u001b[0;36mFBSNN.train\u001b[0;34m(self, N_Iter, learning_rate)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients again to ensure correct gradient accumulation\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the gradients of the loss w.r.t. the network parameters\u001b[39;00m\n\u001b[1;32m    258\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the network parameters based on the gradients\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fbsde_env/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fbsde_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/fbsde_env/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"Algorithms/\"))\n",
    "sys.path.append(os.path.abspath(\"models/\"))\n",
    "#%%\n",
    "from cqf_0_FBSNNs import *\n",
    "from cqf_0_CallOption import *\n",
    "#%%\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "M = 1 # number of trajectories (batch size)\n",
    "N = 50  # number of time snapshots\n",
    "D = 1 # number of dimensions\n",
    "Mm = N ** (1/5)\n",
    "\n",
    "layers = [D + 1] + 4 * [256] + [1]\n",
    "\n",
    "Xi = np.array([1.0] * D)[None, :]\n",
    "T = 1.0\n",
    "\n",
    "\"Available architectures\"\n",
    "mode = \"Naisnet\"  # FC and Naisnet are available\n",
    "activation = \"Sine\"  # Sine, ReLU and Tanh are available\n",
    "model = CallOption(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "n_iter = 2 * 10 ** 4\n",
    "lr = 1e-3\n",
    "#%%\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "#%%\n",
    "# model.load_model(\"models/CallOption4-256XVAPaper.pth\")\n",
    "#%%\n",
    "n_iter = 51 * 10 ** 2\n",
    "lr = 1e-5\n",
    "#%%\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "#%%\n",
    "\n",
    "np.random.seed(37)\n",
    "t_test, W_test = model.fetch_minibatch()\n",
    "X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "\n",
    "if type(t_test).__module__ != 'numpy':\n",
    "    t_test = t_test.cpu().numpy()\n",
    "if type(X_pred).__module__ != 'numpy':\n",
    "    X_pred = X_pred.cpu().detach().numpy()\n",
    "if type(Y_pred).__module__ != 'numpy':\n",
    "    Y_pred = Y_pred.cpu().detach().numpy()\n",
    "\n",
    "for i in range(15):\n",
    "    t_test_i, W_test_i = model.fetch_minibatch()\n",
    "    X_pred_i, Y_pred_i = model.predict(Xi, t_test_i, W_test_i)\n",
    "    if type(X_pred_i).__module__ != 'numpy':\n",
    "        X_pred_i = X_pred_i.cpu().detach().numpy()\n",
    "    if type(Y_pred_i).__module__ != 'numpy':\n",
    "        Y_pred_i = Y_pred_i.cpu().detach().numpy()\n",
    "    if type(t_test_i).__module__ != 'numpy':\n",
    "        t_test_i = t_test_i.cpu().numpy()\n",
    "    t_test = np.concatenate((t_test, t_test_i), axis=0)\n",
    "    X_pred = np.concatenate((X_pred, X_pred_i), axis=0)\n",
    "    Y_pred = np.concatenate((Y_pred, Y_pred_i), axis=0)\n",
    "X_pred = X_pred[:500, :]\n",
    "# %%\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "\n",
    "# %%\n",
    "X_preds = X_pred[:, :, 0]\n",
    "\n",
    "\n",
    "# %%\n",
    "def black_scholes_call(S, K, T, r, sigma, q=0):\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    call_price = (S * np.exp(-q * T) * normal.cdf(d1)) - (K * np.exp(-r * T) * normal.cdf(d2))\n",
    "    delta = normal.cdf(d1)\n",
    "    return call_price, delta\n",
    "\n",
    "\n",
    "def calculate_option_prices(X_pred, time_array, K, r, sigma, T, q=0):\n",
    "    rows, cols = X_pred.shape\n",
    "    option_prices = np.zeros((rows, cols))\n",
    "    deltas = np.zeros((rows, cols))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            S = X_pred[i, j]\n",
    "            t = time_array[j]\n",
    "            time_to_maturity = T - t\n",
    "            if time_to_maturity > 0:\n",
    "                option_prices[i, j], deltas[i, j] = black_scholes_call(S, K, time_to_maturity, r, sigma, q)\n",
    "            else:\n",
    "                option_prices[i, j] = max(S - K, 0)\n",
    "                if S > K:\n",
    "                    deltas[i, j] = 1\n",
    "                elif S == K:\n",
    "                    deltas[i, j] = 0.5\n",
    "                else:\n",
    "                    deltas[i, j] = 0\n",
    "\n",
    "    return option_prices, deltas\n",
    "\n",
    "\n",
    "# Given parameters\n",
    "K = 1.0  # Strike price\n",
    "r = 0.01  # Risk-free interest rate\n",
    "sigma = 0.25  # Volatility\n",
    "q = 0  # Dividend yield (assuming none)\n",
    "T = 1  # Expiry time in years\n",
    "\n",
    "Y_test, Z_test = calculate_option_prices(X_preds, t_test[0], K, r, sigma, T, q)\n",
    "\n",
    "errors = (Y_test[:500] - Y_pred[:500,:,0])**2\n",
    "errors.mean(), errors.std()\n",
    "\n",
    "np.sqrt(errors.mean())\n",
    "\n",
    "graph = model.iteration, model.training_loss\n",
    "#%%\n",
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 438.17227\n",
    "    inches_per_pt = 1.0/72.27\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale\n",
    "    fig_height = nplots*fig_width*golden_mean\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "#%%\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(graph[0], graph[1])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Evolution of the training loss')\n",
    "samples = 5\n",
    "# plt.savefig('Figures/CallOption1DLoss.pdf')\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T)\n",
    "\n",
    "plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T)\n",
    "\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "plt.title(str(D) + '-dimensional Call Option, ' + model.mode + \"-\" + model.activation)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(t_test[0] * 100, Y_pred[0] * 100, 'b', label='Learned $u(t,X_t)$')\n",
    "plt.plot(t_test[0] * 100, Y_test[0] * 100, 'r--', label='Exact $u(t,X_t)$')\n",
    "plt.plot(t_test[0, -1] * 100, Y_test[0, -1] * 100, 'ko', label='$Y_T = u(T,X_T)$')\n",
    "for i in range(7):\n",
    "    plt.plot(t_test[i] * 100, Y_pred[i] * 100, 'b')\n",
    "    plt.plot(t_test[i] * 100, Y_test[i] * 100, 'r--')\n",
    "    plt.plot(t_test[i, -1] * 100, Y_test[i, -1] * 100, 'ko')\n",
    "plt.plot([0], Y_test[0,0] * 100, 'ks', label='$Y_0 = u(0,X_0)$')\n",
    "plt.title(str(D) + '-dimensional Call Option, ' + model.mode + \"-\" + model.activation)\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "plt.savefig(\"CallOption1DPreds.png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbsde_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
