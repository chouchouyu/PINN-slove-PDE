import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional, Dict, Any
import time
import pandas as pd
from scipy import stats
from matplotlib.patches import Patch

# è®¾ç½®éšæœºç§å­
np.random.seed(100)
torch.manual_seed(100)

def rel_error_l2(u, uanal):
    """ç›¸å¯¹L2è¯¯å·®è®¡ç®—"""
    if abs(uanal) >= 10 * np.finfo(float).eps:
        return np.sqrt((u - uanal)**2 / uanal**2)
    else:
        return abs(u - uanal)

class U0Network(nn.Module):
    """u0ç½‘ç»œï¼šè¿‘ä¼¼åˆå§‹è§£å€¼"""
    def __init__(self, d, hls):
        super(U0Network, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(d, hls),
            nn.ReLU(),
            nn.Linear(hls, hls),
            nn.ReLU(),
            nn.Linear(hls, 1)
        )

    def forward(self, x):
        return self.network(x)

class SigmaTGradUNetwork(nn.Module):
    """Ïƒáµ€âˆ‡uç½‘ç»œï¼šæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªç‹¬ç«‹ç½‘ç»œ"""
    def __init__(self, d, hls):
        super(SigmaTGradUNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(d, hls),
            nn.ReLU(),
            nn.Linear(hls, hls),
            nn.ReLU(),
            nn.Linear(hls, hls),
            nn.ReLU(),
            nn.Linear(hls, d)
        )

    def forward(self, x):
        return self.network(x)

class BlackScholesBarenblattSolver:
    """Black-Scholes-Barenblattæ–¹ç¨‹æ±‚è§£å™¨"""
    
    def __init__(self, d=30, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.d = d
        self.device = device
        
        # æ–¹ç¨‹å‚æ•°
        self.r = 0.05
        self.sigma = 0.4
        
        # åˆå§‹æ¡ä»¶å’Œæ—¶é—´è®¾ç½®
        self.x0 = torch.tensor([1.0 if i % 2 == 0 else 0.5 for i in range(d)], 
                              dtype=torch.float32, device=device)
        self.tspan = (0.0, 1.0)
        self.dt = 0.25
        self.time_steps = int((self.tspan[1] - self.tspan[0]) / self.dt)
        self.m = 30  # è®­ç»ƒè½¨è¿¹æ•°
        
        # Legendreå˜æ¢å‚æ•°
        self.A = torch.linspace(-2.0, 2.0, 401, device=device)
        self.u_domain = torch.linspace(-500.0, 500.0, 10001, device=device)
        
        # ç½‘ç»œåˆå§‹åŒ–
        self.hls = 10 + d
        self.u0 = U0Network(d, self.hls).to(device)
        self.sigma_grad_u = nn.ModuleList([
            SigmaTGradUNetwork(d, self.hls).to(device) for _ in range(self.time_steps)
        ])
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            list(self.u0.parameters()) + 
            [param for net in self.sigma_grad_u for param in net.parameters()],
            lr=0.001
        )
        
        # è®­ç»ƒå†å²è®°å½•
        self.losses = []
        self.u0_history = []

    def g(self, X):
        """ç»ˆç«¯æ¡ä»¶ï¼šg(X) = sum(X^2)"""
        return torch.sum(X**2, dim=1, keepdim=True)

    def f(self, X, u, sigma_grad_u, t):
        """éçº¿æ€§é¡¹ï¼šf(X, u, Ïƒáµ€âˆ‡u, p, t) = r * (u - sum(X * Ïƒáµ€âˆ‡u))"""
        return self.r * (u - torch.sum(X * sigma_grad_u, dim=1, keepdim=True))

    def mu_f(self, X, t):
        """æ¼‚ç§»é¡¹ï¼šÎ¼(X, p, t) = 0"""
        return torch.zeros_like(X)

    def sigma_f(self, X, t):
        """æ‰©æ•£é¡¹ï¼šÏƒ(X, p, t) = Diagonal(sigma * X)"""
        if len(X.shape) == 1:
            return torch.diag(self.sigma * X)
        else:
            batch_size = X.shape[0]
            return torch.diag_embed(self.sigma * X)

    def generate_trajectories(self, batch_size=None):
        """ç”Ÿæˆè½¨è¿¹"""
        if batch_size is None:
            batch_size = self.m
            
        X = self.x0.repeat(batch_size, 1)
        u = self.u0(X)
        
        ts = torch.arange(self.tspan[0], self.tspan[1] + self.dt/2, self.dt, device=self.device)
        
        for i in range(len(ts) - 1):
            t = ts[i].item()
            
            sigma_grad_u_val = self.sigma_grad_u[i](X)
            dW = torch.randn(batch_size, self.d, device=self.device) * np.sqrt(self.dt)
            
            # æ›´æ–°u
            f_val = self.f(X, u, sigma_grad_u_val, t)
            u = u - f_val * self.dt + torch.sum(sigma_grad_u_val * dW, dim=1, keepdim=True)
            
            # æ›´æ–°X
            mu_val = self.mu_f(X, t)
            sigma_val = self.sigma_f(X, t)
            
            if len(sigma_val.shape) == 2:
                X = X + mu_val * self.dt + torch.matmul(dW, sigma_val)
            else:
                dW_expanded = dW.unsqueeze(-1)
                X_update = torch.matmul(sigma_val, dW_expanded).squeeze(-1)
                X = X + mu_val * self.dt + X_update
        
        return X, u

    def loss_function(self):
        """æŸå¤±å‡½æ•°"""
        X_final, u_final = self.generate_trajectories()
        g_X = self.g(X_final)
        loss = torch.mean((g_X - u_final) ** 2)
        return loss

    def train(self, maxiters=150, abstol=1e-8, verbose=True):
        """è®­ç»ƒè¿‡ç¨‹"""
        for epoch in range(maxiters):
            self.optimizer.zero_grad()
            loss = self.loss_function()
            loss.backward()
            self.optimizer.step()
            
            self.losses.append(loss.item())
            current_u0 = self.u0(self.x0.unsqueeze(0))[0, 0].item()
            self.u0_history.append(current_u0)
            
            if verbose and (epoch % 10 == 0 or epoch == maxiters - 1):
                print(f'Epoch {epoch}, Loss: {loss.item():.6f}, u0: {current_u0:.6f}')
            
            if loss.item() < abstol:
                if verbose:
                    print(f'Converged at epoch {epoch}')
                break

    def analytical_solution(self, x, t):
        """è§£æè§£"""
        T = self.tspan[1]
        exponent = (self.r + self.sigma**2) * (T - t)
        return torch.exp(torch.tensor(exponent, device=x.device)) * torch.sum(x**2)

    def compute_upper_bound(self, trajectories=1000, maxiters_limits=10, verbose=True):
        """è®¡ç®—ä¸Šç•Œ"""
        if verbose:
            print("Calculating upper bound...")
        
        u0_high = U0Network(self.d, self.hls).to(self.device)
        u0_high.load_state_dict(self.u0.state_dict())
        
        sigma_grad_u_high = nn.ModuleList([
            SigmaTGradUNetwork(self.d, self.hls).to(self.device) for _ in range(self.time_steps)
        ])
        for i, net in enumerate(sigma_grad_u_high):
            net.load_state_dict(self.sigma_grad_u[i].state_dict())
        
        high_opt = optim.Adam(
            list(u0_high.parameters()) + 
            [param for net in sigma_grad_u_high for param in net.parameters()],
            lr=0.01
        )
        
        ts = torch.arange(self.tspan[0], self.tspan[1] + self.dt/2, self.dt, device=self.device)
        
        def upper_bound_loss():
            total = torch.tensor(0.0, device=self.device, requires_grad=True)
            
            for _ in range(trajectories):
                X = self.x0.clone().unsqueeze(0)
                X_trajectory = [X.clone()]
                
                with torch.no_grad():
                    for i in range(len(ts) - 1):
                        t = ts[i].item()
                        dW = torch.randn(1, self.d, device=self.device) * np.sqrt(self.dt)
                        mu_val = self.mu_f(X, t)
                        sigma_val = self.sigma_f(X, t)
                        
                        if len(sigma_val.shape) == 2:
                            X = X + mu_val * self.dt + torch.matmul(dW, sigma_val)
                        else:
                            dW_expanded = dW.unsqueeze(-1)
                            X_update = torch.matmul(sigma_val, dW_expanded).squeeze(-1)
                            X = X + mu_val * self.dt + X_update
                        
                        X_trajectory.append(X.clone())
                
                U = self.g(X)
                
                for i in range(len(ts)-2, -1, -1):
                    t = ts[i].item()
                    X_prev = X_trajectory[i]
                    sigma_grad_u_val = sigma_grad_u_high[i](X_prev)
                    dW = torch.randn(1, self.d, device=self.device) * np.sqrt(self.dt)
                    
                    f_val = self.f(X_prev, U, sigma_grad_u_val, t)
                    U = U + f_val * self.dt - torch.sum(sigma_grad_u_val * dW, dim=1, keepdim=True)
                
                total = total + U
            
            return total / trajectories

        for i in range(maxiters_limits):
            high_opt.zero_grad()
            upper_bound = upper_bound_loss()
            loss = -upper_bound
            loss.backward()
            high_opt.step()
            
            if verbose and (i % 2 == 0 or i == maxiters_limits - 1):
                with torch.no_grad():
                    current_bound = -upper_bound_loss().item()
                print(f'Upper bound optimization {i}: {current_bound:.6f}')
        
        with torch.no_grad():
            final_upper_bound = upper_bound_loss()
            u_high = final_upper_bound.item()
        
        if verbose:
            print(f"Upper bound: {u_high:.6f}")
        
        return u_high

    def compute_lower_bound(self, trajectories=1000, verbose=True):
        """è®¡ç®—ä¸‹ç•Œ"""
        if verbose:
            print("Calculating lower bound with Legendre transform...")
        
        ts = torch.arange(self.tspan[0], self.tspan[1] + self.dt/2, self.dt, device=self.device)
        total_lower = torch.tensor(0.0, device=self.device)
        
        for _ in range(trajectories):
            u = self.u0(self.x0.unsqueeze(0))[0, 0]
            X = self.x0.clone()
            I = torch.tensor(0.0, device=self.device)
            Q = torch.tensor(0.0, device=self.device)
            
            for i in range(len(ts) - 1):
                t = ts[i].item()
                
                sigma_grad_u_val = self.sigma_grad_u[i](X.unsqueeze(0)).squeeze(0)
                dW = torch.randn(self.d, device=self.device) * np.sqrt(self.dt)
                
                X_2d = X.unsqueeze(0)
                u_2d = u.unsqueeze(0).unsqueeze(-1)
                sigma_grad_u_val_2d = sigma_grad_u_val.unsqueeze(0)
                
                f_val = self.f(X_2d, u_2d, sigma_grad_u_val_2d, t)[0, 0]
                dot_product = torch.dot(sigma_grad_u_val, dW)
                u = u - f_val * self.dt + dot_product
                
                mu_val = self.mu_f(X, t)
                sigma_val = self.sigma_f(X, t)
                X_update = torch.matmul(sigma_val, dW.unsqueeze(-1)).squeeze(-1)
                X = X + mu_val * self.dt + X_update
                
                X_dot_sigma_grad_u = torch.sum(X * sigma_grad_u_val)
                f_matrix = self.r * (self.u_domain - X_dot_sigma_grad_u)
                
                a_expanded = self.A.unsqueeze(1)
                u_expanded = self.u_domain.unsqueeze(0)
                f_expanded = f_matrix.unsqueeze(0)
                
                le_matrix = a_expanded * u_expanded - f_expanded
                legendre_values, _ = torch.max(le_matrix, dim=1)
                
                a_u_minus_F = self.A * u - legendre_values
                optimal_idx = torch.argmax(a_u_minus_F)
                a_optimal = self.A[optimal_idx]
                F_optimal = legendre_values[optimal_idx]
                
                I = I + a_optimal * self.dt
                Q = Q + torch.exp(I) * F_optimal
            
            g_X = self.g(X.unsqueeze(0))[0, 0]
            total_lower = total_lower + torch.exp(I) * g_X - Q
        
        u_low = (total_lower / trajectories).item()
        
        if verbose:
            print(f"Lower bound: {u_low:.6f}")
        
        return u_low

    def solve(self, limits=False, trajectories_upper=1000, trajectories_lower=1000, 
              maxiters_limits=10, verbose=True, save_everystep=False):
        """ä¸»æ±‚è§£å‡½æ•°"""
        self.train(verbose=verbose)
        u0_estimate = self.u0(self.x0.unsqueeze(0))[0, 0].item()
        u_analytical = self.analytical_solution(self.x0, self.tspan[0]).item()
        
        if not limits:
            if verbose:
                print(f"Point estimate: {u0_estimate:.6f}")
                print(f"Analytical solution: {u_analytical:.6f}")
                error = rel_error_l2(u0_estimate, u_analytical)
                print(f"Relative error: {error:.6f}")
            
            class PIDESolution:
                def __init__(self, X0, ts, losses, u0_estimate, u0_network, limits=None):
                    self.X0 = X0
                    self.ts = ts
                    self.losses = losses
                    self.us = u0_estimate
                    self.u0 = u0_network
                    self.limits = limits
            
            ts_array = torch.arange(self.tspan[0], self.tspan[1] + self.dt/2, self.dt).cpu().numpy()
            
            if save_everystep:
                return PIDESolution(self.x0.cpu().numpy(), ts_array, self.losses, self.u0_history, self.u0)
            else:
                return PIDESolution(self.x0.cpu().numpy(), ts_array, self.losses, u0_estimate, self.u0)
        
        else:
            u_high = self.compute_upper_bound(
                trajectories=trajectories_upper, 
                maxiters_limits=maxiters_limits, 
                verbose=verbose
            )
            
            u_low = self.compute_lower_bound(
                trajectories=trajectories_lower,
                verbose=verbose
            )
            
            if verbose:
                print(f"\nSolution bounds:")
                print(f"Lower bound: {u_low:.6f}")
                print(f"Point estimate: {u0_estimate:.6f}") 
                print(f"Upper bound: {u_high:.6f}")
                print(f"Analytical solution: {u_analytical:.6f}")
                print(f"Within bounds: {u_low <= u0_estimate <= u_high}")
            
            error = rel_error_l2(u0_estimate, u_analytical)
            
            if verbose:
                print(f"Relative error: {error:.6f}")
            
            class PIDESolution:
                def __init__(self, X0, ts, losses, u0_estimate, u0_network, limits=None):
                    self.X0 = X0
                    self.ts = ts
                    self.losses = losses
                    self.us = u0_estimate
                    self.u0 = u0_network
                    self.limits = limits
            
            ts_array = torch.arange(self.tspan[0], self.tspan[1] + self.dt/2, self.dt).cpu().numpy()
            
            if save_everystep:
                return PIDESolution(self.x0.cpu().numpy(), ts_array, self.losses, self.u0_history, self.u0, (u_low, u_high))
            else:
                return PIDESolution(self.x0.cpu().numpy(), ts_array, self.losses, u0_estimate, self.u0, (u_low, u_high))

def calculate_proper_error_bars(intervals, point_estimates):
    """è®¡ç®—åˆé€‚çš„è¯¯å·®æ¡ - ä¿®å¤ç‰ˆæœ¬"""
    errors_lower = []
    errors_upper = []
    violations = 0
    
    for (low, high), u0 in zip(intervals, point_estimates):
        if u0 < low:
            # ç‚¹ä¼°è®¡ä½äºä¸‹ç•Œï¼Œè°ƒæ•´æ˜¾ç¤º
            errors_lower.append(low - u0)  # ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹å€¼
            errors_upper.append(high - u0)
            violations += 1
        elif u0 > high:
            # ç‚¹ä¼°è®¡é«˜äºä¸Šç•Œ
            errors_lower.append(u0 - low)
            errors_upper.append(u0 - high)  # ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹å€¼
            violations += 1
        else:
            # æ­£å¸¸æƒ…å†µ
            errors_lower.append(u0 - low)
            errors_upper.append(high - u0)
    
    if violations > 0:
        print(f"è­¦å‘Š: {violations}ä¸ªç‚¹ä¼°è®¡è¶…å‡ºç½®ä¿¡åŒºé—´")
    
    return errors_lower, errors_upper, violations

def plot_improved_confidence_intervals(metrics, analytical_value, ax):
    """æ”¹è¿›çš„ç½®ä¿¡åŒºé—´å¯è§†åŒ– - ä¿®å¤ç‰ˆæœ¬"""
    if not metrics.get('limits_intervals'):
        ax.text(0.5, 0.5, 'æ— ç½®ä¿¡åŒºé—´æ•°æ®', 
                transform=ax.transAxes, ha='center', va='center')
        ax.set_title('ç½®ä¿¡åŒºé—´å¯è§†åŒ–')
        return ax
    
    # è®¡ç®—è¯¯å·®æ¡
    errors_lower, errors_upper, violations = calculate_proper_error_bars(
        metrics['limits_intervals'], metrics['limits_u0']
    )
    
    # ç¡®ä¿è¯¯å·®æ¡éè´Ÿ
    errors_lower = np.abs(errors_lower)
    errors_upper = np.abs(errors_upper)
    
    # ç»˜åˆ¶è¯¯å·®æ¡
    x_positions = range(len(metrics['limits_intervals']))
    
    # ä½¿ç”¨ä¸åŒçš„é¢œè‰²æ ‡è®°è¿è§„ç‚¹
    colors = ['red' if (u0 < low or u0 > high) else 'blue' 
             for (low, high), u0 in zip(metrics['limits_intervals'], metrics['limits_u0'])]
    
    # åˆ†åˆ«ç»˜åˆ¶æ­£å¸¸ç‚¹å’Œè¿è§„ç‚¹
    normal_x = []
    normal_y = []
    normal_lower = []
    normal_upper = []
    
    violation_x = []
    violation_y = []
    violation_lower = []
    violation_upper = []
    
    for i, ((low, high), u0, color) in enumerate(zip(metrics['limits_intervals'], metrics['limits_u0'], colors)):
        if color == 'blue':
            normal_x.append(i)
            normal_y.append(u0)
            normal_lower.append(errors_lower[i])
            normal_upper.append(errors_upper[i])
        else:
            violation_x.append(i)
            violation_y.append(u0)
            violation_lower.append(errors_lower[i])
            violation_upper.append(errors_upper[i])
    
    # ç»˜åˆ¶æ­£å¸¸ç‚¹
    if normal_x:
        ax.errorbar(
            normal_x, normal_y, 
            yerr=[normal_lower, normal_upper],
            fmt='o', capsize=5, color='blue', alpha=0.7,
            label='æ­£å¸¸ç‚¹'
        )
    
    # ç»˜åˆ¶è¿è§„ç‚¹
    if violation_x:
        ax.errorbar(
            violation_x, violation_y, 
            yerr=[violation_lower, violation_upper],
            fmt='o', capsize=5, color='red', alpha=0.7,
            label='è¿è§„ç‚¹'
        )
    
    # æ·»åŠ è§£æè§£å‚è€ƒçº¿
    ax.axhline(y=analytical_value, color='green', linestyle='--', 
              label=f'è§£æè§£: {analytical_value:.2f}')
    
    # æ·»åŠ ç½®ä¿¡åŒºé—´èƒŒæ™¯
    for i, (low, high) in enumerate(metrics['limits_intervals']):
        ax.axhspan(low, high, alpha=0.1, color='gray')
    
    ax.set_xlabel('è¯•éªŒæ¬¡æ•°')
    ax.set_ylabel('u0ä¼°è®¡å€¼')
    ax.set_title('ç½®ä¿¡åŒºé—´å¯è§†åŒ–ï¼ˆæ”¹è¿›ç‰ˆï¼‰')
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    coverage = sum(1 for (low, high), u0 in zip(metrics['limits_intervals'], metrics['limits_u0'])
                   if low <= u0 <= high) / len(metrics['limits_intervals']) * 100
    
    stats_text = f'è¦†ç›–ç‡: {coverage:.1f}%\nè¿è§„ç‚¹: {violations}ä¸ª'
    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # æ·»åŠ å›¾ä¾‹
    legend_elements = [
        Patch(facecolor='blue', alpha=0.7, label='æ­£å¸¸ç‚¹'),
        Patch(facecolor='red', alpha=0.7, label='è¿è§„ç‚¹'),
        Patch(facecolor='gray', alpha=0.1, label='ç½®ä¿¡åŒºé—´')
    ]
    ax.legend(handles=legend_elements)
    
    ax.grid(True, alpha=0.3)
    return ax

def comprehensive_comparison(d=30, num_trials=3):
    """å…¨é¢çš„æ–¹æ³•å¯¹æ¯”åˆ†æ - é›†æˆæ–¹æ¡ˆ3æ”¹è¿›"""
    print("=== DeepBSDEæ–¹æ³•å…¨é¢å¯¹æ¯”åˆ†æ ===")
    print(f"ç»´åº¦: {d}ç»´, è¯•éªŒæ¬¡æ•°: {num_trials}")
    print("=" * 60)
    
    results_std = []
    results_limits = []
    
    performance_metrics = {
        'std_errors': [], 'std_times': [], 'std_u0': [],
        'limits_errors': [], 'limits_times': [], 'limits_u0': [],
        'limits_lower': [], 'limits_upper': [], 'limits_intervals': []
    }
    
    for trial in range(num_trials):
        print(f"\n--- è¯•éªŒ {trial + 1}/{num_trials} ---")
        
        # æµ‹è¯•æ ‡å‡†æ–¹æ³•
        start_time = time.time()
        solver_std = BlackScholesBarenblattSolver(d=d)
        result_std = solver_std.solve(limits=False, verbose=False)
        std_time = time.time() - start_time
        
        u_pred_std = result_std.us if hasattr(result_std.us, '__len__') else result_std.us
        u_anal_std = solver_std.analytical_solution(solver_std.x0, solver_std.tspan[0]).item()
        error_std = rel_error_l2(u_pred_std, u_anal_std)
        
        # æµ‹è¯•å¸¦Legendreå˜æ¢æ–¹æ³•
        start_time = time.time()
        solver_limits = BlackScholesBarenblattSolver(d=d)
        result_limits = solver_limits.solve(
            limits=True, 
            trajectories_upper=200,
            trajectories_lower=200,
            maxiters_limits=5,
            verbose=False
        )
        limits_time = time.time() - start_time
        
        u_pred_limits = result_limits.us if hasattr(result_limits.us, '__len__') else result_limits.us
        u_anal_limits = solver_limits.analytical_solution(solver_limits.x0, solver_limits.tspan[0]).item()
        error_limits = rel_error_l2(u_pred_limits, u_anal_limits)
        
        # å­˜å‚¨ç»“æœ
        results_std.append((solver_std, result_std, error_std, std_time))
        results_limits.append((solver_limits, result_limits, error_limits, limits_time))
        
        # å­˜å‚¨æ€§èƒ½æŒ‡æ ‡
        performance_metrics['std_errors'].append(error_std)
        performance_metrics['std_times'].append(std_time)
        performance_metrics['std_u0'].append(u_pred_std)
        
        performance_metrics['limits_errors'].append(error_limits)
        performance_metrics['limits_times'].append(limits_time)
        performance_metrics['limits_u0'].append(u_pred_limits)
        
        if hasattr(result_limits, 'limits') and result_limits.limits is not None:
            u_low, u_high = result_limits.limits
            performance_metrics['limits_lower'].append(u_low)
            performance_metrics['limits_upper'].append(u_high)
            performance_metrics['limits_intervals'].append((u_low, u_high))
        
        print(f"æ ‡å‡†æ–¹æ³• - è¯¯å·®: {error_std:.6f}, æ—¶é—´: {std_time:.2f}s")
        print(f"å¯¹å¶æ–¹æ³• - è¯¯å·®: {error_limits:.6f}, æ—¶é—´: {limits_time:.2f}s")
        if hasattr(result_limits, 'limits') and result_limits.limits is not None:
            print(f"ç½®ä¿¡åŒºé—´: [{u_low:.4f}, {u_high:.4f}]")
    
    # æ€§èƒ½ç»Ÿè®¡åˆ†æ
    print("\n" + "="*60)
    print("                æ€§èƒ½å¯¹æ¯”åˆ†æç»“æœ")
    print("="*60)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_data = []
    
    # å‡†ç¡®æ€§å¯¹æ¯”
    std_error_mean = np.mean(performance_metrics['std_errors'])
    std_error_std = np.std(performance_metrics['std_errors'])
    limits_error_mean = np.mean(performance_metrics['limits_errors'])
    limits_error_std = np.std(performance_metrics['limits_errors'])
    
    # è®¡ç®—æ—¶é—´å¯¹æ¯”
    std_time_mean = np.mean(performance_metrics['std_times'])
    std_time_std = np.std(performance_metrics['std_times'])
    limits_time_mean = np.mean(performance_metrics['limits_times'])
    limits_time_std = np.std(performance_metrics['limits_times'])
    
    # è§£å€¼ç¨³å®šæ€§å¯¹æ¯”
    std_u0_std = np.std(performance_metrics['std_u0'])
    limits_u0_std = np.std(performance_metrics['limits_u0'])
    
    # ç½®ä¿¡åŒºé—´åˆ†æ
    coverage = 0
    if performance_metrics['limits_intervals']:
        analytical_value = results_limits[0][0].analytical_solution(
            results_limits[0][0].x0, results_limits[0][0].tspan[0]).item()
        for interval in performance_metrics['limits_intervals']:
            if interval[0] <= analytical_value <= interval[1]:
                coverage += 1
        coverage_rate = coverage / len(performance_metrics['limits_intervals'])
        interval_widths = [interval[1] - interval[0] for interval in performance_metrics['limits_intervals']]
        avg_interval_width = np.mean(interval_widths) if interval_widths else 0
    else:
        coverage_rate = 0
        avg_interval_width = 0
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    if len(performance_metrics['std_errors']) > 1 and len(performance_metrics['limits_errors']) > 1:
        t_stat, p_value = stats.ttest_ind(performance_metrics['std_errors'], 
                                         performance_metrics['limits_errors'])
    else:
        t_stat, p_value = 0, 1.0
    
    # è¾“å‡ºè¯¦ç»†å¯¹æ¯”è¡¨æ ¼
    print(f"\n{'æŒ‡æ ‡':<25} {'æ ‡å‡†æ–¹æ³•':<15} {'å¯¹å¶æ–¹æ³•':<15} {'ä¼˜åŠ£åˆ†æ':<20}")
    print("-" * 80)
    
    comparison_data.append([
        "å¹³å‡ç›¸å¯¹è¯¯å·®", 
        f"{std_error_mean:.6f} Â± {std_error_std:.6f}", 
        f"{limits_error_mean:.6f} Â± {limits_error_std:.6f}",
        "âœ“ æ ‡å‡†æ–¹æ³•æ›´ä¼˜" if std_error_mean < limits_error_mean else "âœ“ å¯¹å¶æ–¹æ³•æ›´ä¼˜"
    ])
    
    comparison_data.append([
        "å¹³å‡è®­ç»ƒæ—¶é—´(s)", 
        f"{std_time_mean:.2f} Â± {std_time_std:.2f}", 
        f"{limits_time_mean:.2f} Â± {limits_time_std:.2f}",
        "âœ“ æ ‡å‡†æ–¹æ³•æ›´å¿«" if std_time_mean < limits_time_mean else "âœ“ å¯¹å¶æ–¹æ³•æ›´å¿«"
    ])
    
    comparison_data.append([
        "è§£å€¼ç¨³å®šæ€§(æ ‡å‡†å·®)", 
        f"{std_u0_std:.6f}", 
        f"{limits_u0_std:.6f}",
        "âœ“ æ ‡å‡†æ–¹æ³•æ›´ç¨³å®š" if std_u0_std < limits_u0_std else "âœ“ å¯¹å¶æ–¹æ³•æ›´ç¨³å®š"
    ])
    
    if performance_metrics['limits_intervals']:
        comparison_data.append([
            "ç½®ä¿¡åŒºé—´è¦†ç›–ç‡", 
            "N/A", 
            f"{coverage_rate*100:.1f}%",
            "âœ“ è‰¯å¥½" if coverage_rate >= 0.9 else "â—‹ ä¸€èˆ¬" if coverage_rate >= 0.7 else "âœ— è¾ƒå·®"
        ])
        
        comparison_data.append([
            "å¹³å‡åŒºé—´å®½åº¦", 
            "N/A", 
            f"{avg_interval_width:.4f}",
            "âœ“ è¾ƒçª„" if avg_interval_width < 1.0 else "â—‹ é€‚ä¸­" if avg_interval_width < 3.0 else "âœ— è¾ƒå®½"
        ])
    
    comparison_data.append([
        "ç»Ÿè®¡æ˜¾è‘—æ€§(på€¼)", 
        "N/A", 
        f"{p_value:.6f}",
        "âœ“ æ˜¾è‘—å·®å¼‚" if p_value < 0.05 else "â—‹ æ— æ˜¾è‘—å·®å¼‚"
    ])
    
    for row in comparison_data:
        print(f"{row[0]:<25} {row[1]:<15} {row[2]:<15} {row[3]:<20}")
    
    # ç»˜åˆ¶ç»¼åˆå¯¹æ¯”å›¾è¡¨ï¼ˆåŒ…å«æ–¹æ¡ˆ3æ”¹è¿›çš„ç½®ä¿¡åŒºé—´å¯è§†åŒ–ï¼‰
    plot_comprehensive_comparison(performance_metrics, results_std, results_limits, d)
    
    # æ–¹æ³•ç‰¹æ€§è¯„åˆ†
    print("\n" + "="*60)
    print("                æ–¹æ³•ç‰¹æ€§ç»¼åˆè¯„åˆ†")
    print("="*60)
    
    characteristics = {
        'å‡†ç¡®æ€§': [max(0, 10 - std_error_mean*100), max(0, 10 - limits_error_mean*100)],
        'è®¡ç®—æ•ˆç‡': [max(0, 10 - std_time_mean/10), max(0, 10 - limits_time_mean/10)],
        'æ•°å€¼ç¨³å®šæ€§': [max(0, 10 - std_u0_std*10), max(0, 10 - limits_u0_std*10)],
        'ç†è®ºä¿è¯': [7, 9],
        'å®ç°å¤æ‚åº¦': [8, 6],
        'é€‚ç”¨æ€§': [9, 8]
    }
    
    if performance_metrics['limits_intervals']:
        characteristics['ä¸ç¡®å®šæ€§é‡åŒ–'] = [5, 8]
    
    methods = ['æ ‡å‡†æ–¹æ³•', 'å¯¹å¶æ–¹æ³•']
    print(f"{'ç‰¹æ€§':<15} {'æ ‡å‡†æ–¹æ³•':<10} {'å¯¹å¶æ–¹æ³•':<10} {'æ¨è':<10}")
    print("-" * 50)
    
    for char, scores in characteristics.items():
        std_score, limits_score = scores
        recommendation = "æ ‡å‡†æ–¹æ³•" if std_score > limits_score else "å¯¹å¶æ–¹æ³•" if limits_score > std_score else "ç›¸å½“"
        print(f"{char:<15} {std_score:<10.1f} {limits_score:<10.1f} {recommendation:<10}")
    
    # æ€»ä½“æ¨è
    total_std = sum([score[0] for score in characteristics.values()])
    total_limits = sum([score[1] for score in characteristics.values()])
    
    print("-" * 50)
    print(f"{'æ€»åˆ†':<15} {total_std:<10.1f} {total_limits:<10.1f} ", end="")
    if total_std > total_limits:
        print("âœ“ æ¨èæ ‡å‡†æ–¹æ³•")
    elif total_limits > total_std:
        print("âœ“ æ¨èå¯¹å¶æ–¹æ³•")
    else:
        print("â—‹ ä¸¤ç§æ–¹æ³•ç›¸å½“")
    
    return results_std, results_limits, performance_metrics

def plot_comprehensive_comparison(metrics, results_std, results_limits, d):
    """ç»˜åˆ¶ç»¼åˆå¯¹æ¯”å›¾è¡¨ - é›†æˆæ–¹æ¡ˆ3æ”¹è¿›"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. è¯¯å·®åˆ†å¸ƒå¯¹æ¯”
    axes[0,0].boxplot([metrics['std_errors'], metrics['limits_errors']], 
                      labels=['æ ‡å‡†æ–¹æ³•', 'å¯¹å¶æ–¹æ³•'])
    axes[0,0].set_ylabel('ç›¸å¯¹è¯¯å·®')
    axes[0,0].set_title('è¯¯å·®åˆ†å¸ƒå¯¹æ¯”')
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. è®­ç»ƒæ—¶é—´å¯¹æ¯”
    axes[0,1].boxplot([metrics['std_times'], metrics['limits_times']], 
                     labels=['æ ‡å‡†æ–¹æ³•', 'å¯¹å¶æ–¹æ³•'])
    axes[0,1].set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
    axes[0,1].set_title('è®¡ç®—æ•ˆç‡å¯¹æ¯”')
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. è§£å€¼ç¨³å®šæ€§
    axes[0,2].plot(metrics['std_u0'], 'bo-', label='æ ‡å‡†æ–¹æ³•', alpha=0.7)
    axes[0,2].plot(metrics['limits_u0'], 'ro-', label='å¯¹å¶æ–¹æ³•', alpha=0.7)
    analytical_value = results_std[0][0].analytical_solution(
        results_std[0][0].x0, results_std[0][0].tspan[0]).item()
    axes[0,2].axhline(y=analytical_value, color='green', linestyle='--', 
                     label=f'è§£æè§£: {analytical_value:.2f}')
    axes[0,2].set_xlabel('è¯•éªŒæ¬¡æ•°')
    axes[0,2].set_ylabel('u0ä¼°è®¡å€¼')
    axes[0,2].set_title('è§£å€¼ç¨³å®šæ€§å¯¹æ¯”')
    axes[0,2].legend()
    axes[0,2].grid(True, alpha=0.3)
    
    # 4. è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”ï¼ˆæœ€åä¸€æ¬¡è¯•éªŒï¼‰
    if results_std and hasattr(results_std[-1][0], 'losses'):
        axes[1,0].semilogy(results_std[-1][0].losses, label='æ ‡å‡†æ–¹æ³•')
    if results_limits and hasattr(results_limits[-1][0], 'losses'):
        axes[1,0].semilogy(results_limits[-1][0].losses, label='å¯¹å¶æ–¹æ³•')
    axes[1,0].set_xlabel('è¿­ä»£æ¬¡æ•°')
    axes[1,0].set_ylabel('æŸå¤±å€¼')
    axes[1,0].set_title('è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)
    
    # 5. ç½®ä¿¡åŒºé—´å¯è§†åŒ– - ä½¿ç”¨æ–¹æ¡ˆ3æ”¹è¿›æ–¹æ³•
    if metrics.get('limits_intervals'):
        plot_improved_confidence_intervals(metrics, analytical_value, axes[1,1])
    else:
        axes[1,1].text(0.5, 0.5, 'æ— ç½®ä¿¡åŒºé—´æ•°æ®', 
                      transform=axes[1,1].transAxes, ha='center', va='center')
        axes[1,1].set_title('ç½®ä¿¡åŒºé—´å¯è§†åŒ–')
    
    # 6. æ–¹æ³•ç‰¹æ€§é›·è¾¾å›¾ - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æåæ ‡è®¾ç½®
    # é¦–å…ˆåˆ›å»ºä¸€ä¸ªæåæ ‡è½´
    fig.delaxes(axes[1,2])  # åˆ é™¤åŸæ¥çš„è½´
    ax_radar = fig.add_subplot(2, 3, 6, projection='polar')  # åˆ›å»ºæåæ ‡è½´
    
    characteristics = ['å‡†ç¡®æ€§', 'æ•ˆç‡', 'ç¨³å®šæ€§', 'ç†è®ºä¿è¯', 'æ˜“ç”¨æ€§', 'é€‚ç”¨æ€§']
    std_scores = [8, 9, 7, 7, 8, 9]
    limits_scores = [7, 6, 8, 9, 6, 8]
    
    angles = np.linspace(0, 2*np.pi, len(characteristics), endpoint=False).tolist()
    angles += angles[:1]
    std_scores += std_scores[:1]
    limits_scores += limits_scores[:1]
    characteristics += characteristics[:1]
    
    ax_radar.plot(angles, std_scores, 'o-', linewidth=2, label='æ ‡å‡†æ–¹æ³•')
    ax_radar.fill(angles, std_scores, alpha=0.25)
    ax_radar.plot(angles, limits_scores, 'o-', linewidth=2, label='å¯¹å¶æ–¹æ³•')
    ax_radar.fill(angles, limits_scores, alpha=0.25)
    ax_radar.set_xticks(angles[:-1])
    ax_radar.set_xticklabels(characteristics[:-1])
    ax_radar.set_title('æ–¹æ³•ç‰¹æ€§é›·è¾¾å›¾')
    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    
    plt.tight_layout()
    plt.suptitle(f'{d}ç»´Black-Scholes-Barenblattæ–¹ç¨‹æ±‚è§£æ–¹æ³•å¯¹æ¯”', fontsize=16, y=1.02)
    plt.show()

def main():
    """ä¿®æ”¹åçš„ä¸»å‡½æ•°ï¼Œä¸“æ³¨äºæ–¹æ³•å¯¹æ¯”"""
    print("=== DeepBSDEæ–¹æ³•å¯¹æ¯”åˆ†æ ===")
    
    # è¿è¡Œå…¨é¢å¯¹æ¯”åˆ†æ
    results_std, results_limits, metrics = comprehensive_comparison(d=30, num_trials=3)
    
    # è¾“å‡ºæœ€ç»ˆå»ºè®®
    print("\n" + "="*60)
    print("                æœ€ç»ˆä½¿ç”¨å»ºè®®")
    print("="*60)
    
    print("\nğŸ“Š åŸºäºå¯¹æ¯”åˆ†æï¼Œå»ºè®®å¦‚ä¸‹ï¼š")
    print("\nâœ… æ¨èä½¿ç”¨æ ‡å‡†DeepBSDEæ–¹æ³•çš„æƒ…å†µï¼š")
    print("   â€¢ éœ€è¦å¿«é€Ÿå¾—åˆ°ç‚¹ä¼°è®¡")
    print("   â€¢ è®¡ç®—èµ„æºæœ‰é™")
    print("   â€¢ é—®é¢˜ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–")
    print("   â€¢ å®ç°å¤æ‚åº¦è¦æ±‚ä½")
    
    print("\nâœ… æ¨èä½¿ç”¨å¸¦Legendreå˜æ¢å¯¹å¶æ–¹æ³•çš„æƒ…å†µï¼š")
    print("   â€¢ éœ€è¦ç½®ä¿¡åŒºé—´ä¼°è®¡")
    print("   â€¢ å¯¹è§£çš„å¯é æ€§è¦æ±‚é«˜")
    print("   â€¢ æœ‰å……è¶³çš„è®¡ç®—èµ„æº")
    print("   â€¢ éœ€è¦è¿›è¡Œä¸¥æ ¼çš„ç†è®ºåˆ†æ")
    
    print("\nğŸ” å…³é”®å‘ç°ï¼š")
    if metrics['std_errors'] and metrics['limits_errors']:
        if np.mean(metrics['std_errors']) < np.mean(metrics['limits_errors']):
            print("   â€¢ æ ‡å‡†æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šç•¥ä¼˜")
        else:
            print("   â€¢ å¯¹å¶æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šç•¥ä¼˜")
        
        if np.mean(metrics['std_times']) < np.mean(metrics['limits_times']):
            print("   â€¢ æ ‡å‡†æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜æ˜¾æ›´ä¼˜")
        else:
            print("   â€¢ å¯¹å¶æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ›´ä¼˜")
    
    if metrics.get('limits_intervals'):
        analytical_value = results_limits[0][0].analytical_solution(
            results_limits[0][0].x0, results_limits[0][0].tspan[0]).item()
        coverage = sum(1 for interval in metrics['limits_intervals'] 
                      if interval[0] <= analytical_value <= interval[1])
        print(f"   â€¢ å¯¹å¶æ–¹æ³•æä¾›ç½®ä¿¡åŒºé—´ï¼Œè¦†ç›–ç‡ä¸º{coverage/len(metrics['limits_intervals'])*100:.1f}%")
    
    return results_std, results_limits, metrics

if __name__ == "__main__":
    results_std, results_limits, metrics = main()
