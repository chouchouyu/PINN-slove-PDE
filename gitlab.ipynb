{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2050b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用MPS设备: mps\n",
      "设备类型: mps\n",
      "批量大小: 128\n",
      "累积步数: 1\n",
      "系统总内存: 16.00 GB\n",
      "系统可用内存: 3.91 GB\n",
      "系统使用率: 75.6%\n",
      "开始第一阶段训练...\n",
      "迭代: 0, 损失: 6.589e+01, Y0: 0.071, 时间: 2.33s, 学习率: 4.099e-05, 设备: MPS\n",
      "迭代: 100, 损失: 1.449e+01, Y0: 0.067, 时间: 3.95s, 学习率: 9.674e-04, 设备: MPS\n",
      "迭代: 200, 损失: 1.011e+01, Y0: 0.080, 时间: 3.75s, 学习率: 7.439e-04, 设备: MPS\n",
      "迭代: 300, 损失: 1.026e+01, Y0: 0.153, 时间: 3.74s, 学习率: 4.063e-04, 设备: MPS\n",
      "迭代: 400, 损失: 8.252e+00, Y0: 0.103, 时间: 4.03s, 学习率: 1.125e-04, 设备: MPS\n",
      "第一阶段训练完成，总时间: 22.34s\n",
      "开始第二阶段训练...\n",
      "迭代: 400, 损失: 9.255e+00, Y0: 0.112, 时间: 0.04s, 学习率: 4.281e-07, 设备: MPS\n",
      "迭代: 500, 损失: 8.810e+00, Y0: 0.115, 时间: 3.75s, 学习率: 8.346e-06, 设备: MPS\n",
      "迭代: 600, 损失: 8.950e+00, Y0: 0.106, 时间: 3.62s, 学习率: 2.913e-06, 设备: MPS\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 期权定价的深度随机神经网络\n",
    "# \n",
    "# 本Notebook整合了基于Forward-Backward Stochastic Neural Networks (FBSNNs)的期权定价模型，支持苹果M芯片(MPS)、TPU和GPU加速。\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. 导入必要的库和设备设置\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# 抑制警告\n",
    "warnings.filterwarnings('ignore')\n",
    "#-------- todo: continue from here --------\n",
    "# 设备检测和设置\n",
    "def setup_device():\n",
    "    \"\"\"自动检测并设置最佳计算设备\"\"\"\n",
    "    # 检测TPU\n",
    "    if 'COLAB_TPU_ADDR' in os.environ:\n",
    "        try:\n",
    "            import torch_xla\n",
    "            import torch_xla.core.xla_model as xm\n",
    "            device = xm.xla_device()\n",
    "            print(f\"使用TPU设备: {device}\")\n",
    "            return device, 'tpu'\n",
    "        except ImportError:\n",
    "            print(\"TPU检测到但torch_xla未安装，使用默认设备\")\n",
    "    \n",
    "    # 检测MPS (苹果芯片)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"使用MPS设备: {device}\")\n",
    "        return device, 'mps'\n",
    "    \n",
    "    # 检测CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"使用CUDA设备: {device}\")\n",
    "        return device, 'cuda'\n",
    "    \n",
    "    # 默认CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"使用CPU设备: {device}\")\n",
    "    return device, 'cpu'\n",
    "\n",
    "# 设置设备\n",
    "device, device_type = setup_device()\n",
    "\n",
    "# TPU特定设置\n",
    "if device_type == 'tpu':\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "# %% [markdown]\n",
    "# ## 2. 定义激活函数和神经网络结构\n",
    "\n",
    "# %%\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"正弦激活函数\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sine, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "class Naisnet(nn.Module):\n",
    "    \"\"\"NAIS-Net神经网络结构 - 针对加速设备优化\"\"\"\n",
    "    def __init__(self, layers, stable=True, activation=None, device=None):\n",
    "        super(Naisnet, self).__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.device = device\n",
    "        self.layer1 = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "        self.layer2 = nn.Linear(in_features=layers[1], out_features=layers[2])\n",
    "        self.layer2_input = nn.Linear(in_features=layers[0], out_features=layers[2])\n",
    "        self.layer3 = nn.Linear(in_features=layers[2], out_features=layers[3])\n",
    "        \n",
    "        if len(layers) == 5:\n",
    "            self.layer3_input = nn.Linear(in_features=layers[0], out_features=layers[3])\n",
    "            self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "        elif len(layers) == 6:\n",
    "            self.layer3_input = nn.Linear(in_features=layers[0], out_features=layers[3])\n",
    "            self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "            self.layer4_input = nn.Linear(in_features=layers[0], out_features=layers[4])\n",
    "            self.layer5 = nn.Linear(in_features=layers[4], out_features=layers[5])\n",
    "\n",
    "        self.activation = activation\n",
    "        self.epsilon = 0.01\n",
    "        self.stable = stable\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._init_weights()\n",
    "        \n",
    "        # 移动到设备\n",
    "        self.to(device)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def project(self, layer, out):\n",
    "        \"\"\"NAIS-Net的投影层 - 针对TPU/MPS优化\"\"\"\n",
    "        weights = layer.weight\n",
    "        delta = 1 - 2 * self.epsilon\n",
    "        RtR = torch.matmul(weights.t(), weights)\n",
    "        norm = torch.norm(RtR)\n",
    "        if norm > delta:\n",
    "            RtR = delta ** (1 / 2) * RtR / (norm ** (1 / 2))\n",
    "        \n",
    "        A = RtR + torch.eye(RtR.shape[0], device=RtR.device) * self.epsilon\n",
    "        return F.linear(out, -A, layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.project(self.layer2, out)\n",
    "            out = out + self.layer2_input(u)\n",
    "        else:\n",
    "            out = self.layer2(out)\n",
    "        out = self.activation(out)\n",
    "        out = out + shortcut\n",
    "\n",
    "        if len(self.layers) == 4:\n",
    "            out = self.layer3(out)\n",
    "            return out\n",
    "\n",
    "        if len(self.layers) == 5:\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer3, out)\n",
    "                out = out + self.layer3_input(u)\n",
    "            else:\n",
    "                out = self.layer3(out)\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "            out = self.layer4(out)\n",
    "            return out\n",
    "        \n",
    "        if len(self.layers) == 6:\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer3, out)\n",
    "                out = out + self.layer3_input(u)\n",
    "            else:\n",
    "                out = self.layer3(out)\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer4, out)\n",
    "                out = out + self.layer4_input(u)\n",
    "            else:\n",
    "                out = self.layer4(out)\n",
    "\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "            out = self.layer5(out)\n",
    "            return out\n",
    "\n",
    "        return out\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. 定义FBSNN基类（支持多设备加速）\n",
    "\n",
    "# %%\n",
    "class FBSNN(ABC):\n",
    "    \"\"\"Forward-Backward Stochastic Neural Network基类 - 多设备支持\"\"\"\n",
    "    \n",
    "    def __init__(self, Xi, T, M, N, D, Mm, layers, mode, activation):\n",
    "        # 设备设置\n",
    "        self.device = device\n",
    "        self.device_type = device_type\n",
    "        \n",
    "        # 初始化条件\n",
    "        self.Xi = torch.from_numpy(Xi).float().to(self.device)\n",
    "        self.Xi.requires_grad = True\n",
    "\n",
    "        # 存储参数\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.Mm = Mm\n",
    "        self.strike = 1.0 * self.D\n",
    "\n",
    "        self.mode = mode\n",
    "        self.activation = activation\n",
    "        \n",
    "        # 设置激活函数\n",
    "        if activation == \"Sine\":\n",
    "            self.activation_function = Sine()\n",
    "        elif activation == \"ReLU\":\n",
    "            self.activation_function = nn.ReLU()\n",
    "        elif activation == \"Tanh\":\n",
    "            self.activation_function = nn.Tanh()\n",
    "\n",
    "        # 初始化神经网络\n",
    "        if self.mode == \"FC\":\n",
    "            self.layers_list = []\n",
    "            for i in range(len(layers) - 2):\n",
    "                self.layers_list.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "                self.layers_list.append(self.activation_function)\n",
    "            self.layers_list.append(nn.Linear(in_features=layers[-2], out_features=layers[-1]))\n",
    "            self.model = nn.Sequential(*self.layers_list).to(self.device)\n",
    "\n",
    "        elif self.mode == \"Naisnet\":\n",
    "            self.model = Naisnet(layers, stable=True, activation=self.activation_function, device=self.device)\n",
    "\n",
    "        # 训练记录\n",
    "        self.training_loss = []\n",
    "        self.iteration = []\n",
    "        \n",
    "        # 性能优化标志\n",
    "        self.optimize_for_device()\n",
    "\n",
    "    def optimize_for_device(self):\n",
    "        \"\"\"根据设备类型进行特定优化\"\"\"\n",
    "        if self.device_type == 'tpu':\n",
    "            # TPU优化设置\n",
    "            torch.set_grad_enabled(True)\n",
    "            # 启用XLA动态形状（提高TPU性能）\n",
    "            os.environ['XLA_USE_BF16'] = '1'\n",
    "            os.environ['XLA_USE_F16'] = '1'\n",
    "            \n",
    "        elif self.device_type == 'mps':\n",
    "            # MPS优化设置\n",
    "            # 注意：torch.backends.mps没有set_memory_efficient方法\n",
    "            # 可以使用以下MPS特定的优化\n",
    "            \n",
    "            # 启用自动混合精度（如果可用）\n",
    "            if hasattr(torch, 'autocast'):\n",
    "                # 这将用于在后续训练中使用混合精度\n",
    "                self.use_amp = True\n",
    "            else:\n",
    "                self.use_amp = False\n",
    "            \n",
    "            # 清空MPS缓存\n",
    "            if hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "                \n",
    "        elif self.device_type == 'cuda':\n",
    "            # CUDA优化设置\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            self.use_amp = True  # 启用自动混合精度\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        \"\"\"权重初始化\"\"\"\n",
    "        if type(m) == nn.Linear:\n",
    "            if self.device_type == 'tpu':\n",
    "                # TPU上使用更稳定的初始化\n",
    "                torch.nn.init.xavier_normal_(m.weight)\n",
    "            else:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def net_u(self, t, X):\n",
    "        \"\"\"神经网络前向传播 - 设备优化版本\"\"\"\n",
    "        # 使用torch.cat的优化版本\n",
    "        input = torch.cat((t, X), 1)\n",
    "        u = self.model(input)\n",
    "        \n",
    "        # 梯度计算优化\n",
    "        if u.requires_grad:\n",
    "            Du = torch.autograd.grad(outputs=u, inputs=X, \n",
    "                                   grad_outputs=torch.ones_like(u),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "        else:\n",
    "            # 如果不需要梯度，创建零张量\n",
    "            Du = torch.zeros_like(X)\n",
    "        return u, Du\n",
    "\n",
    "    def Dg_tf(self, X):\n",
    "        \"\"\"计算g函数的梯度 - 优化版本\"\"\"\n",
    "        g = self.g_tf(X)\n",
    "        if g.requires_grad:\n",
    "            Dg = torch.autograd.grad(outputs=g, inputs=X, \n",
    "                                   grad_outputs=torch.ones_like(g),\n",
    "                                   create_graph=True, retain_graph=True)[0]\n",
    "        else:\n",
    "            Dg = torch.zeros_like(X)\n",
    "        return Dg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fetch_minibatch(self):\n",
    "        \"\"\"生成小批量数据 - 设备优化版本\"\"\"\n",
    "        T = self.T\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        D = self.D\n",
    "\n",
    "        # 预分配内存\n",
    "        Dt = np.zeros((M, N + 1, 1), dtype=np.float32)\n",
    "        DW = np.zeros((M, N + 1, D), dtype=np.float32)\n",
    "\n",
    "        dt = T / N\n",
    "        Dt[:, 1:, :] = dt\n",
    "        \n",
    "        # 向量化随机数生成\n",
    "        DW[:, 1:, :] = np.sqrt(dt) * np.random.randn(M, N, D).astype(np.float32)\n",
    "\n",
    "        t = np.cumsum(Dt, axis=1)\n",
    "        W = np.cumsum(DW, axis=1)\n",
    "\n",
    "        # 直接创建在目标设备上\n",
    "        t_tensor = torch.from_numpy(t).float().to(self.device)\n",
    "        W_tensor = torch.from_numpy(W).float().to(self.device)\n",
    "\n",
    "        return t_tensor, W_tensor\n",
    "\n",
    "    def loss_function(self, t, W, Xi, training=True):\n",
    "        \"\"\"计算损失函数 - 性能优化版本\"\"\"\n",
    "        if training:\n",
    "            loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=self.device, requires_grad=False)\n",
    "            \n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "\n",
    "        t0 = t[:, 0, :]\n",
    "        W0 = W[:, 0, :]\n",
    "        X0 = Xi.repeat(self.M, 1).view(self.M, self.D)\n",
    "        Y0, Z0 = self.net_u(t0, X0)\n",
    "\n",
    "        X_list.append(X0)\n",
    "        Y_list.append(Y0)\n",
    "\n",
    "        # 使用更高效的内存管理\n",
    "        for n in range(self.N):\n",
    "            t1 = t[:, n + 1, :]\n",
    "            W1 = W[:, n + 1, :]\n",
    "            \n",
    "            # 向量化计算\n",
    "            mu = self.mu_tf(t0, X0, Y0, Z0)\n",
    "            sigma = self.sigma_tf(t0, X0, Y0)\n",
    "            dW = (W1 - W0).unsqueeze(-1)\n",
    "            \n",
    "            # 使用einsum提高计算效率\n",
    "            sigma_dW = torch.einsum('mij,mj->mi', sigma, dW.squeeze(-1))\n",
    "            X1 = X0 + mu * (t1 - t0) + sigma_dW\n",
    "            \n",
    "            Z_sigma_dW = torch.sum(Z0 * sigma_dW, dim=1, keepdim=True)\n",
    "            Y1_tilde = Y0 + self.phi_tf(t0, X0, Y0, Z0) * (t1 - t0) + Z_sigma_dW\n",
    "            \n",
    "            Y1, Z1 = self.net_u(t1, X1)\n",
    "            \n",
    "            if training:\n",
    "                loss = loss + torch.sum((Y1 - Y1_tilde) ** 2)\n",
    "            else:\n",
    "                loss = loss + torch.sum((Y1 - Y1_tilde) ** 2).detach()\n",
    "\n",
    "            # 更新变量（避免不必要的拷贝）\n",
    "            t0, W0, X0, Y0, Z0 = t1, W1, X1, Y1, Z1\n",
    "            X_list.append(X0)\n",
    "            Y_list.append(Y0)\n",
    "\n",
    "        # 终端条件损失\n",
    "        if training:\n",
    "            terminal_loss = torch.sum((Y1 - self.g_tf(X1)) ** 2)\n",
    "            gradient_loss = torch.sum((Z1 - self.Dg_tf(X1)) ** 2)\n",
    "            loss = loss + terminal_loss + gradient_loss\n",
    "        else:\n",
    "            terminal_loss = torch.sum((Y1 - self.g_tf(X1)) ** 2).detach()\n",
    "            gradient_loss = torch.sum((Z1 - self.Dg_tf(X1)) ** 2).detach()\n",
    "            loss = loss + terminal_loss + gradient_loss\n",
    "\n",
    "        X = torch.stack(X_list, dim=1)\n",
    "        Y = torch.stack(Y_list, dim=1)\n",
    "\n",
    "        return loss, X, Y, Y[0, 0, 0]\n",
    "\n",
    "    def train(self, N_Iter, learning_rate, accumulation_steps=1):\n",
    "        \"\"\"训练模型 - 多设备优化版本\"\"\"\n",
    "        loss_temp = []\n",
    "        previous_it = 0 if not self.iteration else self.iteration[-1]\n",
    "\n",
    "        # 设备特定的优化器设置\n",
    "        if self.device_type == 'tpu':\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, \n",
    "                                       weight_decay=1e-4, fused=True)\n",
    "        else:\n",
    "            self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, \n",
    "                                       weight_decay=1e-4)\n",
    "        \n",
    "        # 学习率调度器\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer, max_lr=learning_rate, \n",
    "            total_steps=N_Iter, pct_start=0.1\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        gradient_accumulation_counter = 0\n",
    "        \n",
    "        for it in range(previous_it, previous_it + N_Iter):\n",
    "            # 动态调整时间步数\n",
    "            if it >= 4000 and it < 20000:\n",
    "                self.N = int(np.ceil(self.Mm ** (int(it / 4000) + 1)))\n",
    "            elif it < 4000:\n",
    "                self.N = int(np.ceil(self.Mm))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            t_batch, W_batch = self.fetch_minibatch()\n",
    "            \n",
    "            # 使用自动混合精度（如果可用）\n",
    "            if hasattr(self, 'use_amp') and self.use_amp and self.device_type != 'tpu':\n",
    "                with torch.autocast(device_type=self.device_type if self.device_type != 'mps' else 'cpu', dtype=torch.float16):\n",
    "                    loss, X_pred, Y_pred, Y0_pred = self.loss_function(t_batch, W_batch, self.Xi, training=True)\n",
    "            else:\n",
    "                loss, X_pred, Y_pred, Y0_pred = self.loss_function(t_batch, W_batch, self.Xi, training=True)\n",
    "            \n",
    "            # 梯度累积\n",
    "            scaled_loss = loss / accumulation_steps\n",
    "            scaled_loss.backward()\n",
    "            gradient_accumulation_counter += 1\n",
    "            \n",
    "            if gradient_accumulation_counter == accumulation_steps:\n",
    "                if self.device_type == 'tpu':\n",
    "                    # TPU特定的梯度裁剪\n",
    "                    xm.optimizer_step(self.optimizer)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                gradient_accumulation_counter = 0\n",
    "                \n",
    "                # 清空MPS缓存（如果使用MPS设备）\n",
    "                if self.device_type == 'mps' and it % 100 == 0:\n",
    "                    if hasattr(torch.mps, 'empty_cache'):\n",
    "                        torch.mps.empty_cache()\n",
    "\n",
    "            # 设备特定的损失记录\n",
    "            if self.device_type == 'tpu':\n",
    "                loss_value = loss.detach().cpu().item()\n",
    "            else:\n",
    "                loss_value = loss.detach().item()\n",
    "                \n",
    "            loss_temp.append(loss_value)\n",
    "            \n",
    "            # 进度报告\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                if self.device_type == 'tpu':\n",
    "                    print(f'迭代: {it}, 损失: {loss_value:.3e}, Y0: {Y0_pred:.3f}, '\n",
    "                          f'时间: {elapsed:.2f}s, 学习率: {current_lr:.3e}, 设备: TPU')\n",
    "                else:\n",
    "                    print(f'迭代: {it}, 损失: {loss_value:.3e}, Y0: {Y0_pred:.3f}, '\n",
    "                          f'时间: {elapsed:.2f}s, 学习率: {current_lr:.3e}, 设备: {self.device_type.upper()}')\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "            # 记录损失\n",
    "            if it % 100 == 0:\n",
    "                avg_loss = np.mean(loss_temp)\n",
    "                self.training_loss.append(avg_loss)\n",
    "                loss_temp = []\n",
    "                self.iteration.append(it)\n",
    "                \n",
    "                # TPU特定操作\n",
    "                if self.device_type == 'tpu' and it % 1000 == 0:\n",
    "                    xm.mark_step()\n",
    "        \n",
    "        # 最终标记步骤（TPU）\n",
    "        if self.device_type == 'tpu':\n",
    "            xm.mark_step()\n",
    "                \n",
    "        return np.stack((self.iteration, self.training_loss))\n",
    "\n",
    "    def predict(self, Xi_star, t_star, W_star):\n",
    "        \"\"\"预测 - 设备优化版本\"\"\"\n",
    "        Xi_star = torch.from_numpy(Xi_star).float().to(self.device)\n",
    "        Xi_star.requires_grad = False  # 预测时不需要梯度\n",
    "        \n",
    "        # 设置模型为评估模式\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, X_star, Y_star, _ = self.loss_function(t_star, W_star, Xi_star, training=False)\n",
    "        \n",
    "        # 恢复训练模式\n",
    "        self.model.train()\n",
    "        \n",
    "        return X_star, Y_star\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        \"\"\"保存模型 - 多设备兼容\"\"\"\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'training_loss': self.training_loss,\n",
    "            'iteration': self.iteration,\n",
    "            'device_type': self.device_type\n",
    "        }\n",
    "        torch.save(state, file_name)\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        \"\"\"加载模型 - 多设备兼容\"\"\"\n",
    "        checkpoint = torch.load(file_name, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.training_loss = checkpoint['training_loss']\n",
    "        self.iteration = checkpoint['iteration']\n",
    "\n",
    "    @abstractmethod\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def g_tf(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.zeros([M, D], device=self.device)\n",
    "\n",
    "    @abstractmethod\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.diag_embed(torch.ones([M, D], device=self.device))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. 定义看涨期权类\n",
    "\n",
    "# %%\n",
    "class CallOption(FBSNN):\n",
    "    \"\"\"看涨期权定价模型 - 多设备优化版本\"\"\"\n",
    "    \n",
    "    def __init__(self, Xi, T, M, N, D, Mm, layers, mode, activation):\n",
    "        super().__init__(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        \"\"\"漂移项\"\"\"\n",
    "        rate = 0.01\n",
    "        return rate * Y\n",
    "\n",
    "    def g_tf(self, X):\n",
    "        \"\"\"终端条件\"\"\"\n",
    "        temp = torch.sum(X, dim=1, keepdim=True)\n",
    "        return torch.maximum(temp - self.strike, torch.tensor(0.0, device=self.device))\n",
    "\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        \"\"\"漂移系数\"\"\"\n",
    "        rate = 0.01\n",
    "        return rate * X\n",
    "\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        \"\"\"扩散系数\"\"\"\n",
    "        sigma = 0.25\n",
    "        return sigma * torch.diag_embed(X)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. 辅助函数\n",
    "\n",
    "# %%\n",
    "def black_scholes_call(S, K, T, r, sigma, q=0):\n",
    "    \"\"\"Black-Scholes看涨期权定价公式\"\"\"\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    if T <= 0:\n",
    "        return max(S - K, 0), 1.0 if S > K else 0.0\n",
    "    \n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    call_price = (S * np.exp(-q * T) * norm.cdf(d1)) - (K * np.exp(-r * T) * norm.cdf(d2))\n",
    "    delta = norm.cdf(d1)\n",
    "    return call_price, delta\n",
    "\n",
    "def calculate_option_prices(X_pred, time_array, K, r, sigma, T, q=0):\n",
    "    \"\"\"计算期权价格和Delta\"\"\"\n",
    "    rows, cols = X_pred.shape\n",
    "    option_prices = np.zeros((rows, cols))\n",
    "    deltas = np.zeros((rows, cols))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            S = X_pred[i, j]\n",
    "            t = time_array[j]\n",
    "            time_to_maturity = T - t\n",
    "            option_prices[i, j], deltas[i, j] = black_scholes_call(S, K, time_to_maturity, r, sigma, q)\n",
    "\n",
    "    return option_prices, deltas\n",
    "\n",
    "def figsize(scale, nplots=1):\n",
    "    \"\"\"设置图形大小\"\"\"\n",
    "    fig_width_pt = 438.17227\n",
    "    inches_per_pt = 1.0 / 72.27\n",
    "    golden_mean = (np.sqrt(5.0) - 1.0) / 2.0\n",
    "    fig_width = fig_width_pt * inches_per_pt * scale\n",
    "    fig_height = nplots * fig_width * golden_mean\n",
    "    return [fig_width, fig_height]\n",
    "\n",
    "def check_device_memory():\n",
    "    \"\"\"检查设备内存使用情况\"\"\"\n",
    "    if device_type == 'cuda':\n",
    "        print(f\"GPU内存使用: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        print(f\"GPU内存缓存: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "    elif device_type == 'mps':\n",
    "        # MPS设备内存信息不可直接获取，但我们可以检查系统内存\n",
    "        import psutil\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        print(f\"系统总内存: {memory_info.total/1024**3:.2f} GB\")\n",
    "        print(f\"系统可用内存: {memory_info.available/1024**3:.2f} GB\")\n",
    "        print(f\"系统使用率: {memory_info.percent}%\")\n",
    "    elif device_type == 'tpu':\n",
    "        print(\"TPU设备内存信息不可直接获取\")\n",
    "    else:\n",
    "        import psutil\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        print(f\"系统总内存: {memory_info.total/1024**3:.2f} GB\")\n",
    "        print(f\"系统可用内存: {memory_info.available/1024**3:.2f} GB\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. 模型训练和测试\n",
    "\n",
    "# %%\n",
    "# 设置参数（根据设备类型调整）\n",
    "if device_type == 'tpu':\n",
    "    M = 512  # TPU适合大批量\n",
    "    accumulation_steps = 4\n",
    "elif device_type == 'cuda':\n",
    "    M = 256  # GPU中等批量\n",
    "    accumulation_steps = 2\n",
    "elif device_type == 'mps':\n",
    "    M = 128  # MPS较小批量\n",
    "    accumulation_steps = 1\n",
    "else:\n",
    "    M = 64   # CPU小批量\n",
    "    accumulation_steps = 1\n",
    "\n",
    "N = 50\n",
    "D = 1\n",
    "Mm = N ** (1/5)\n",
    "\n",
    "layers = [D + 1] + 4 * [256] + [1]\n",
    "\n",
    "Xi = np.array([1.0] * D)[None, :]\n",
    "T = 1.0\n",
    "\n",
    "mode = \"Naisnet\"\n",
    "activation = \"Sine\"\n",
    "\n",
    "print(f\"设备类型: {device_type}\")\n",
    "print(f\"批量大小: {M}\")\n",
    "print(f\"累积步数: {accumulation_steps}\")\n",
    "\n",
    "# 创建模型\n",
    "model = CallOption(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "# %%\n",
    "# 检查设备内存\n",
    "check_device_memory()\n",
    "\n",
    "# %%\n",
    "# 第一阶段训练\n",
    "print(\"开始第一阶段训练...\")\n",
    "n_iter = 500  # 减少迭代次数用于演示\n",
    "lr = 1e-3\n",
    "\n",
    "tot = time.time()\n",
    "graph = model.train(n_iter, lr, accumulation_steps)\n",
    "print(f\"第一阶段训练完成，总时间: {time.time() - tot:.2f}s\")\n",
    "\n",
    "# %%\n",
    "# 第二阶段训练（精细调优）\n",
    "print(\"开始第二阶段训练...\")\n",
    "n_iter = 300  # 减少迭代次数用于演示\n",
    "lr = 1e-5\n",
    "\n",
    "tot = time.time()\n",
    "graph = model.train(n_iter, lr, accumulation_steps)\n",
    "print(f\"第二阶段训练完成，总时间: {time.time() - tot:.2f}s\")\n",
    "\n",
    "# 测试模型\n",
    "print(\"开始测试...\")\n",
    "np.random.seed(37)\n",
    "t_test, W_test = model.fetch_minibatch()\n",
    "X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "\n",
    "# 转换为numpy数组\n",
    "if device_type == 'tpu':\n",
    "    t_test = t_test.cpu().numpy()\n",
    "    X_pred = X_pred.cpu().numpy()\n",
    "    Y_pred = Y_pred.cpu().numpy()\n",
    "else:\n",
    "    t_test = t_test.cpu().numpy()\n",
    "    X_pred = X_pred.detach().cpu().numpy()\n",
    "    Y_pred = Y_pred.detach().cpu().numpy()\n",
    "\n",
    "# 收集测试数据\n",
    "test_samples = 2 if device_type == 'tpu' else 3  # 减少测试样本\n",
    "for i in range(test_samples):\n",
    "    t_test_i, W_test_i = model.fetch_minibatch()\n",
    "    X_pred_i, Y_pred_i = model.predict(Xi, t_test_i, W_test_i)\n",
    "    \n",
    "    if device_type == 'tpu':\n",
    "        t_test_i = t_test_i.cpu().numpy()\n",
    "        X_pred_i = X_pred_i.cpu().numpy()\n",
    "        Y_pred_i = Y_pred_i.cpu().numpy()\n",
    "    else:\n",
    "        t_test_i = t_test_i.cpu().numpy()\n",
    "        X_pred_i = X_pred_i.detach().cpu().numpy()\n",
    "        Y_pred_i = Y_pred_i.detach().cpu().numpy()\n",
    "        \n",
    "    t_test = np.concatenate((t_test, t_test_i), axis=0)\n",
    "    X_pred = np.concatenate((X_pred, X_pred_i), axis=0)\n",
    "    Y_pred = np.concatenate((Y_pred, Y_pred_i), axis=0)\n",
    "\n",
    "X_pred = X_pred[:50]  # 限制数据量\n",
    "\n",
    "# 计算Black-Scholes基准值\n",
    "from scipy.stats import norm  # 导入norm函数\n",
    "K = 1.0\n",
    "r = 0.01\n",
    "sigma = 0.25\n",
    "q = 0\n",
    "T = 1\n",
    "\n",
    "# 确保X_preds形状正确\n",
    "X_preds = X_pred[:, :, 0] if len(X_pred.shape) == 3 else X_pred\n",
    "Y_test, Z_test = calculate_option_prices(X_preds, t_test[0, :, 0], K, r, sigma, T, q)\n",
    "\n",
    "# 确保Y_pred形状正确\n",
    "Y_pred_reshaped = Y_pred[:, :, 0] if len(Y_pred.shape) == 3 else Y_pred\n",
    "errors = (Y_test[:50] - Y_pred_reshaped[:50])**2\n",
    "print(f\"均方误差: {errors.mean():.6f}\")\n",
    "print(f\"误差标准差: {errors.std():.6f}\")\n",
    "print(f\"均方根误差: {np.sqrt(errors.mean()):.6f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. 结果可视化\n",
    "\n",
    "# %%\n",
    "# 绘制训练损失\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(graph[0], graph[1])\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失值')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('训练损失变化')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 绘制预测结果对比\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "samples = min(5, len(t_test))\n",
    "\n",
    "# 绘制学习到的解\n",
    "for i in range(samples):\n",
    "    plt.plot(t_test[i, :, 0], Y_pred_reshaped[i, :], 'b-', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# 绘制精确解\n",
    "for i in range(samples):\n",
    "    plt.plot(t_test[i, :, 0], Y_test[i, :], 'r--', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.xlabel('时间 $t$')\n",
    "plt.ylabel('期权价格 $Y_t = u(t,X_t)$')\n",
    "plt.title(f'{D}维看涨期权 - {model.mode}-{model.activation}')\n",
    "plt.legend(['学习解', '精确解'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 详细对比图\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "\n",
    "samples_plot = min(7, len(t_test))\n",
    "for i in range(samples_plot):\n",
    "    plt.plot(t_test[i, :, 0] * 100, Y_pred_reshaped[i, :] * 100, 'b-', alpha=0.6, linewidth=1.5)\n",
    "    plt.plot(t_test[i, :, 0] * 100, Y_test[i, :] * 100, 'r--', alpha=0.6, linewidth=1.5)\n",
    "    plt.plot(t_test[i, -1, 0] * 100, Y_test[i, -1] * 100, 'ko', markersize=4)\n",
    "\n",
    "plt.plot([0], Y_test[0, 0] * 100, 'ks', markersize=6, label='$Y_0 = u(0,X_0)$')\n",
    "plt.plot(t_test[0, -1, 0] * 100, Y_test[0, -1] * 100, 'ko', markersize=6, label='$Y_T = u(T,X_T)$')\n",
    "\n",
    "plt.title(f'{D}维看涨期权 - {model.mode}-{model.activation}')\n",
    "plt.legend(['学习解', '精确解', '$Y_0$', '$Y_T$'])\n",
    "plt.xlabel('时间 $t$ (%)')\n",
    "plt.ylabel('期权价格 $Y_t = u(t,X_t)$')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 误差分布图\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "absolute_errors = np.abs(Y_test[:50] - Y_pred_reshaped[:50])\n",
    "plt.hist(absolute_errors.flatten(), bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.xlabel('绝对误差')\n",
    "plt.ylabel('频数')\n",
    "plt.title('预测误差分布')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 性能统计\n",
    "print(\"\\n=== 性能统计 ===\")\n",
    "print(f\"设备类型: {device_type}\")\n",
    "print(f\"批量大小: {M}\")\n",
    "print(f\"时间步数: {N}\")\n",
    "print(f\"训练迭代: {len(model.iteration)} 次\")\n",
    "print(f\"最终损失: {model.training_loss[-1]:.6f}\")\n",
    "print(f\"测试RMSE: {np.sqrt(errors.mean()):.6f}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save_model(f'call_option_model_{device_type}.pth')\n",
    "print(f\"模型已保存为: call_option_model_{device_type}.pth\")\n",
    "\n",
    "print(\"\\n所有代码执行完成！\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. 多设备性能对比（可选）\n",
    "\n",
    "# %%\n",
    "def benchmark_model():\n",
    "    \"\"\"基准测试函数\"\"\"\n",
    "    print(\"开始基准测试...\")\n",
    "    \n",
    "    # 测试推理速度\n",
    "    test_iterations = 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(test_iterations):\n",
    "        t_test, W_test = model.fetch_minibatch()\n",
    "        X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "        \n",
    "        if device_type == 'tpu':\n",
    "            xm.mark_step()  # TPU需要标记步骤\n",
    "    \n",
    "    inference_time = (time.time() - start_time) / test_iterations\n",
    "    print(f\"平均推理时间: {inference_time:.4f} 秒/次\")\n",
    "    \n",
    "    # 测试训练速度\n",
    "    if len(model.training_loss) > 10:\n",
    "        print(f\"最后10次平均损失: {np.mean(model.training_loss[-10:]):.6f}\")\n",
    "\n",
    "# 运行基准测试\n",
    "benchmark_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
