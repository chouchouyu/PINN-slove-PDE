2.6.2 Training and Optimization

The training of a neural network is an optimization task. Its goal is to find the set of parameters $\theta$ that minimizes a loss function, $\mathcal{J}(\theta)$. The form of this function is problem-specific.

The minimization is typically achieved using gradient-based algorithms. A standard approach is gradient descent, or one of its variants, where the parameters are updated at each training iteration $k$ according to the rule:

$$
\theta_{k+1} = \theta_{k} - \alpha \nabla_{\theta} \mathcal{J}(\theta_{k}),
$$

where $\alpha > 0$ is the learning rate.